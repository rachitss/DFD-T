{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b2088ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7f95d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         filename  label  split        original  \\\n",
      "0  aagfhgtpmv.mp4      1  train  vudstovrck.mp4   \n",
      "1  aapnvogymq.mp4      1  train  jdubbvfswz.mp4   \n",
      "2  abarnvbtwb.mp4      0  train             NaN   \n",
      "3  abqwwspghj.mp4      1  train  qzimuostzz.mp4   \n",
      "4  acifjvzvpm.mp4      1  train  kbvibjhfzo.mp4   \n",
      "\n",
      "                                                path  \n",
      "0  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aag...  \n",
      "1  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aap...  \n",
      "2  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aba...  \n",
      "3  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\abq...  \n",
      "4  D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\aci...  \n"
     ]
    }
   ],
   "source": [
    "valid_train_df = pd.read_csv('valid_train_df.csv')\n",
    "# Add index to DataFrame\n",
    "print(valid_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083ec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 count: 76, Label 1 count: 306\n"
     ]
    }
   ],
   "source": [
    "# # Analysis of values\n",
    "\n",
    "# label_counts = valid_train_df['label'].value_counts()\n",
    "# label_0_count = label_counts.get(0, 0)\n",
    "# label_1_count = label_counts.get(1, 0)\n",
    "# print(f\"Label 0 count: {label_0_count}, Label 1 count: {label_1_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b36b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\W\\VS\\VS Folder\\DFD\\DFDC MTCNN Extracted\\cizlkenljw , 0\n"
     ]
    }
   ],
   "source": [
    "row=valid_train_df.iloc[192]\n",
    "print(row['path'],',', row['label'])\n",
    "path= row['path']\n",
    "label = row['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07fbb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'vit_backbone.pth'\n",
    "classifier_path = 'vit_classifier_head.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d95aae6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and processor\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "model = model.to(device)\n",
    "classifier = nn.Linear(768, 1)\n",
    "classifier = classifier.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee3b0a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=1, bias=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "classifier.load_state_dict(torch.load(classifier_path))\n",
    "model = model.to(device)\n",
    "classifier = classifier.to(device)\n",
    "model.eval()\n",
    "classifier.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5dc7b490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d004fab8119d4c2ba0541e63957a2c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Predicted class: 1, Actual class: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 36\n",
    "pred_classes = []\n",
    "total_batches = 0\n",
    "\n",
    "image_files = sorted([ os.path.join(path, f)\n",
    "for f in os.listdir(path)\n",
    "if f.lower().endswith('.png')\n",
    "])\n",
    "\n",
    "for i in tqdm(range(0, len(image_files), batch_size), desc='Image Batches', leave=False):\n",
    "    total_batches += 1\n",
    "    batch_paths = image_files[i:i+batch_size]\n",
    "    images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "    inputs = processor(images=images, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # (batch, hidden_dim)\n",
    "        outputs = classifier(cls_embeddings)  # (batch, 1)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        pred_classes.extend(preds.cpu().numpy().flatten().tolist())\n",
    "\n",
    "pred = int(sum(pred_classes) > len(pred_classes) / 2)\n",
    "display(f\"Predicted class: {pred}, Actual class: {label}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
